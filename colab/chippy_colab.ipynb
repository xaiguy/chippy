{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda48f81-cdcd-4edb-9694-240e310aec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown # Dependencies\n",
    "\n",
    "from IPython.utils import capture\n",
    "import time\n",
    "import os\n",
    "\n",
    "print('\u001bInstalling dependencies...')\n",
    "with capture.capture_output() as cap:\n",
    "    %cd /content/\n",
    "    !git clone https://github.com/xaiguy/chippy\n",
    "    %cd chippy\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install accelerate\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "print('\u001bDone, proceed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e8701-3aca-4541-b3c6-a61c19267ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "class Data(BaseModel):\n",
    "    input_prompt: str\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"Rallio67/chip_1.4B_instruct_alpha\",\n",
    "    device_map=\"auto\", \n",
    "    #load_in_8bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Rallio67/chip_1.4B_instruct_alpha\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fff02-681c-49e7-82b8-eae3c1597b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(prompt):\n",
    "    inputs = tokenizer(\"User: \" + prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    tokens = model.generate(**inputs, \n",
    "                            top_p=0.95,\n",
    "                            temperature=0.5,\n",
    "                            top_k=4, \n",
    "                            repetition_penalty=1.03,\n",
    "                            max_length=100,\n",
    "                            early_stopping=True\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(tokens[0])\n",
    "    return output.replace(\"<|endoftext|>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2d912-83b1-4c6c-a7bb-a52702bf4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le's test it\n",
    "prompt=\"Hello, what are you?\"\n",
    "reply = model_predict(prompt)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f5f39-789d-48ed-96bd-086ca17b1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown # Writing Streamlit app file\n",
    "\n",
    "%%writefile app.py\n",
    "# Imports\n",
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "import requests\n",
    "import regex as re\n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "# Create our simple data structure\n",
    "class Data(BaseModel):\n",
    "    input_prompt: str\n",
    "\n",
    "# Load the model\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"Rallio67/chip_1.4B_instruct_alpha\",\n",
    "    device_map=\"auto\", \n",
    "    #load_in_8bit=True\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Rallio67/chip_1.4B_instruct_alpha\"\n",
    ")\n",
    "\n",
    "# Define prediction function\n",
    "def model_predict(prompt):\n",
    "    inputs = tokenizer(\"User: \" + prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    tokens = model.generate(**inputs, \n",
    "                            top_p=0.95,\n",
    "                            temperature=0.5,\n",
    "                            top_k=4, \n",
    "                            repetition_penalty=1.03,\n",
    "                            max_length=100,\n",
    "                            early_stopping=True\n",
    "    )\n",
    "\n",
    "    output = tokenizer.decode(tokens[0])\n",
    "    return output.replace(\"<|endoftext|>\", \"\")\n",
    "\n",
    "# Streamlit App Title\n",
    "st.title(\"Chippy Google Colab\")\n",
    "\n",
    "# Streamlit input field\n",
    "input_prompt = st.text_input(\"Enter a prompt\", \"What is a Large Language Model?\")\n",
    "\n",
    "placeholder = st.empty()  # placeholder for latest message\n",
    "message_history = []\n",
    "message_history.append(input_prompt)\n",
    "\n",
    "for j, message_ in enumerate(message_history):\n",
    "    if j % 2 == 0:\n",
    "        message(message_, is_user=True) # display all the previous message\n",
    "\n",
    "res = model_predict(prompt=input_prompt)\n",
    "cleaned_answer = re.sub(\"User:.+\\n+Chip: \", \"\", res)\n",
    "message(cleaned_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03dac06-3da7-42ec-b574-1269b8cdda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might take some time since we're loading the model IN the app, normally we would use an API for that\n",
    "!streamlit run app.py > /dev/null & npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
